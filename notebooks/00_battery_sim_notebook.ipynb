{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e373ece",
   "metadata": {},
   "source": [
    "<div style=\"font-size:200%;font-weight:bold\">Energy Storage System</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0a2f9",
   "metadata": {},
   "source": [
    "This notebook demontrates how to train an RL agent for Energy Storage System (ESS) arbitrage. The simulated energy environment is created based on the paper [Arbitrage of Energy Storage in Electricity Markets with Deep Reinforcement Learning](https://arxiv.org/abs/1904.12232), and with [this sample dataset](https://aemo.com.au/en/energy-systems/electricity/national-electricity-market-nem/data-nem/aggregated-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ed149",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a982d2d",
   "metadata": {},
   "source": [
    "Ensure that your Python virtual environment have installed they energy storage system python package `pip install -e .`. Then, execute the next cell to download the sample data to a a local file called `data/sample-data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!bash download_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ff89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "files = glob.glob(\"data/PRICE_AND_DEMAND*.csv\")\n",
    "df = pd.concat([pd.read_csv(f) for f in files], axis=0, ignore_index=True)\n",
    "df.sort_values('SETTLEMENTDATE', inplace=True)\n",
    "df.to_csv(\"data/sample-data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd297a8f",
   "metadata": {},
   "source": [
    "# Global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74034782",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "from energy_storage_system.agents import MovingAveragePriceAgent, PriceVsCostAgent, RandomAgent\n",
    "from energy_storage_system.envs import SimpleBattery\n",
    "from energy_storage_system.utils import evaluate_episode, plot_reward, plot_analysis, train\n",
    "\n",
    "env_config = {\n",
    "    \"MAX_STEPS_PER_EPISODE\": 168,\n",
    "    \"LOCAL\": True,  # True means to use data from local src folder instead of S3.\n",
    "    \"FILEPATH\": \"data/sample-data.csv\"\n",
    "}\n",
    "env = SimpleBattery(env_config)\n",
    "episodes = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1a404",
   "metadata": {},
   "source": [
    "The next cell defines a helper function `train_eval()` to (train + evaluate + plot) an agent. This function will be used to evaluate three baseline agents:\n",
    "\n",
    "1. a random agent\n",
    "2. an agent that considers market price vs cost\n",
    "3. an agent that considers the moving average of market price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(env, agent, episodes) -> pd.DataFrame:\n",
    "    \"\"\"Helper function to train, evaluate, and plot.\"\"\"\n",
    "    # Training\n",
    "    train_results = train(env, agent, episodes)\n",
    "    plot_reward(train_results.rewards_list)  # Jupyter autoplots the returned fig\n",
    "    print(\"Average rewards across training episodes:\", train_results.mean_rewards)\n",
    "\n",
    "    # Evaluation\n",
    "    df_eval = evaluate_episode(agent, env)\n",
    "    plot_analysis(df_eval)  # Jupyter autoplots the returned fig\n",
    "\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ea022",
   "metadata": {},
   "source": [
    "# Random Agent\n",
    "\n",
    "Train an agent who behaves randomly.\n",
    "\n",
    "**Policy evaluation and observation**: the agent action is totally random, regardless of price and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df_eval_random = train_eval(env, RandomAgent(), episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f455a82",
   "metadata": {},
   "source": [
    "# Market price vs cost agent\n",
    "\n",
    "This agent behaves as follows:\n",
    "\n",
    "- SELL: when market price is higher than cost\n",
    "- BUY: when market price is lower than cost\n",
    "- HOLD: others\n",
    "\n",
    "**Policy evaluation and observation**: agent discharges (sell:1) when price is higher than cost, and charges (buy:0)\n",
    "\n",
    "    CHARGE = 0\n",
    "    DISCHARGE = 1\n",
    "    HOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df_eval_price_vs_cost = train_eval(env, PriceVsCostAgent(), episodes)\n",
    "\n",
    "# Save the evaluation episode.\n",
    "df_eval_price_vs_cost.to_csv(\"result_price_vs_cost_agent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607bfa5",
   "metadata": {},
   "source": [
    "# Market Price vs Historical price Agent\n",
    "\n",
    "This agent behaves as follows:\n",
    "\n",
    "- SELL: when market price is higher than past 5 days average price\n",
    "- BUY: when market price is lower than past 5 days average price\n",
    "- HOLD: others\n",
    "\n",
    "**Policy evaluation and observation**: Agent will start selling when market price is increasing (high than last 5 days average), and buy when market price is dropping.\n",
    "\n",
    "    CHARGE = 0\n",
    "    DISCHARGE = 1\n",
    "    HOLD = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dae61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df_eval_ma = train_eval(env, MovingAveragePriceAgent(), episodes)\n",
    "\n",
    "# Save the evaluation episode.\n",
    "df_eval_ma.to_csv(\"result_hist_price_agent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5838f",
   "metadata": {},
   "source": [
    "# SageMaker RL - DQN\n",
    "\n",
    "Next is to use DQN algorithm running on SageMaker RL. Please refer to separate notebook for more info.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_custom_p37",
   "language": "python",
   "name": "conda_custom_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
